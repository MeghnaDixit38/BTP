{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MeghnaDixit38/BTP/blob/main/2d_coupled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1eKu-7qskXz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.autograd as ag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiyRcPBqyJqB"
      },
      "outputs": [],
      "source": [
        "pi = np.pi\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBPzrtUxvovs"
      },
      "source": [
        "Solving the PDE: \\\\\n",
        "$$-u'(x) = f(x)$$\n",
        "Given: $$u (x \\rightarrow \\infty) = 0$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExgAhVXijGue"
      },
      "source": [
        "eq: \\\\\n",
        "$$e^{-\\lambda} \\left(\\frac{\\nu^{'}}{r} + \\frac{1}{r^2} \\right) - \\frac{1}{r^2} = 0$$\n",
        "$$e^{-\\lambda} \\left(\\frac{\\lambda^{'}}{r} - \\frac{1}{r^2} \\right) - \\frac{1}{r^2} = 0$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juHZQpgOxsE8"
      },
      "source": [
        "Rewritten as:\n",
        "$$ \\nu^{'} = \\frac{e^{\\lambda} - 1}{r}  $$\n",
        "$$ \\lambda^{'} = \\frac{e^{\\lambda} + 1}{r}  $$\n",
        "\n",
        "Substituting $$ f = e^{-\\lambda} $$\n",
        "$$ g = e^{\\nu} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeI7f5L7u8tM"
      },
      "outputs": [],
      "source": [
        "## Exact solution\n",
        "def g_exact(xt):\n",
        "  g = (1-1/xt)\n",
        "  return g\n",
        "\n",
        "def f_exact(xt):\n",
        "  f = (1-1/xt)\n",
        "  return f\n",
        "\n",
        "## Derivative\n",
        "def f_dot(f , xt):\n",
        "    return -(1+f)/xt\n",
        "\n",
        "def g_dot(f, g , xt):\n",
        "    return g*(1/f -1)/xt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACegSh4pkImZ"
      },
      "source": [
        "Exact: \n",
        "$$ e^{-\\lambda} = e^{\\nu} = 1 - \\frac{1}{r} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBonxL_LwZ73"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qErE1MkXyjTe"
      },
      "outputs": [],
      "source": [
        "class data(Dataset):\n",
        "  def __init__(self, x):\n",
        "    self.ts = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    xs = self.x[idx]\n",
        "    return xs\n",
        "\n",
        "def tensor(x):\n",
        "    return torch.tensor(x, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xv-KJDA1Qe-"
      },
      "source": [
        "### DNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJYxRSGd7h3a"
      },
      "outputs": [],
      "source": [
        "class DNN(nn.Module):\n",
        "    def __init__(self, dim_in=1, dim_out=2,\n",
        "                 n_layers=2, n_neurons=10, activation=nn.Sigmoid()):\n",
        "        super(DNN, self).__init__()\n",
        "\n",
        "        self.dim_in = dim_in\n",
        "        self.dim_out = dim_out\n",
        "        self.n_layers = n_layers\n",
        "        self.n_neurons = n_neurons\n",
        "        self.activation = activation\n",
        "\n",
        "        layers = [nn.Linear(dim_in, n_neurons), activation]\n",
        "        for i in range(n_layers - 2):\n",
        "            layers.extend([nn.Linear(n_neurons, n_neurons), activation])\n",
        "        layers.extend([nn.Linear(n_neurons, dim_out)])\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, xt):\n",
        "        return self.layers(xt)\n",
        "\n",
        "def _D1(u, x, idx_u=0):\n",
        "    Du = ag.grad(\n",
        "        u[:, idx_u], x, torch.ones_like(u[:, idx_u]),\n",
        "        create_graph=True\n",
        "    )[0]\n",
        "    return Du\n",
        "\n",
        "def _D2(u, x, idx_u=0, idx_x=0):\n",
        "    Du = ag.grad(\n",
        "        u[:, idx_u], x, torch.ones_like(u[:, idx_u]),\n",
        "        create_graph=True\n",
        "    )[0]\n",
        "    grad_out = torch.zeros_like(Du)\n",
        "    grad_out[:, idx_x] = 1.0\n",
        "    D2u = ag.grad(Du, x, grad_out, create_graph=True)[0]\n",
        "    return D2u\n",
        "\n",
        "class PhysicsLoss:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def __call__(self, xs):\n",
        "        '''\n",
        "        This function needs to be implemented for each problem.\n",
        "        '''\n",
        "        raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-yIXrJS1ehZ"
      },
      "source": [
        "### Residual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb14lPRVWXvZ"
      },
      "outputs": [],
      "source": [
        "class UnitInterval(Dataset):\n",
        "    def __init__(self, n_interior=1000, left = 1.0, right = 1000.0):\n",
        "        h = (right-left)/n_interior\n",
        "        self.n_interior = n_interior\n",
        "        xs = np.linspace(left + h/5, right - h/5, n_interior)\n",
        "        self.xs = xs.reshape((n_interior, 1))\n",
        "        self.xbs_l = np.array([left]).reshape((1, 1))\n",
        "        self.xbs_r = np.array([right]).reshape((1, 1))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_interior\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.xs[idx]\n",
        "\n",
        "\n",
        "class Residual_PINN(PhysicsLoss):\n",
        "    def __init__(self, model, data, bdy_wt_u =1.0, bdy_wt_v =1.0):\n",
        "      super().__init__(model)\n",
        "\n",
        "      self.model = model\n",
        "      self.data = data\n",
        "      # self.fn = fn\n",
        "      # self.gn = gn\n",
        "      self.bdy_wt_u = bdy_wt_u\n",
        "      self.bdy_wt_v = bdy_wt_v\n",
        "\n",
        "    def _squared_residual(self, x, u , ux, v, vx, vxx):\n",
        "      ### both loss models giving same result\n",
        "        return (u*(vxx/v - 0.5*(vx/v)**2 + (vx/v + ux/u)/x + (vx*ux)/(u*v)))**2\n",
        "      # return (ux - self.fn(u , x))**2 + (vx - self.gn(u, v, x))**2\n",
        "\n",
        "    def _interior_loss(self, xs):\n",
        "      u = self.model(xs)[:,0].unsqueeze(1)\n",
        "      v = self.model(xs)[:,1].unsqueeze(1)\n",
        "      # print('u=',u, u.size())\n",
        "      D1u = _D1(u, xs)\n",
        "      D2u = _D2(u, xs)\n",
        "      D1v = _D1(v, xs)\n",
        "      D2v = _D2(v, xs)\n",
        "      loss_int = torch.mean(self._squared_residual(xs, u , D1u, v, D1v, D2v))\n",
        "      return loss_int\n",
        "\n",
        "\n",
        "    def _boundary_loss(self, u, xs, bc_u = 1.0, bc_v = 1.0):\n",
        "      loss_bdy = torch.sum((u-bc_u)**2)\n",
        "      return loss_bdy\n",
        "\n",
        "    def __call__(self, xs):\n",
        "      loss = self._interior_loss(xs)\n",
        "\n",
        "      if self.bdy_wt_u > 1e-6:\n",
        "        # For 1st order PDE\n",
        "        u = self.model(xs)[:,0].unsqueeze(1)\n",
        "        v = self.model(xs)[:,1].unsqueeze(1)\n",
        "        loss +=  self.bdy_wt_u*self._boundary_loss(u, tensor(self.data.xbs_r)) + self.bdy_wt_v*self._boundary_loss(v, tensor(self.data.xbs_r))\n",
        "        # For 2nd order PDE\n",
        "        # loss +=  self.bdy_wt_r*self._boundary_loss(tensor(self.data.xbs_r)) + self.bdy_wt_l*self._boundary_loss(tensor(self.data.xbs_l), bc = 0.0)\n",
        "      return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwgHJJ0bC4LD"
      },
      "source": [
        "### Ritz and L2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0u4hNQkDC7o0"
      },
      "outputs": [],
      "source": [
        "class Residual_Deep_Ritz(PhysicsLoss):\n",
        "    def __init__(self, model, data, fn, bdy_wt_l =1.0, bdy_wt_r =1.0):\n",
        "      super().__init__(model)\n",
        "\n",
        "      self.model = model\n",
        "      self.data = data\n",
        "      self.fn = fn\n",
        "      self.bdy_wt_l = bdy_wt_l\n",
        "      self.bdy_wt_r = bdy_wt_r\n",
        "\n",
        "    def _squared_residual(self, x, u , ux):\n",
        "      return abs((0.5*(ux)**2 - self.fn(u , x)*u)) \n",
        "\n",
        "    def _interior_loss(self, xs):\n",
        "      u = self.model(xs)\n",
        "      D1u = _D1(u, xs)\n",
        "      D2u = _D2(u, xs)\n",
        "      loss_int = torch.mean(self._squared_residual(xs, u , D1u))\n",
        "      return loss_int\n",
        "\n",
        "    def _boundary_loss(self, xs, bc = 1.0):\n",
        "      u = self.model(xs)\n",
        "      loss_bdy = torch.sum((u-bc)**2)\n",
        "      return loss_bdy\n",
        "\n",
        "    def __call__(self, xs):\n",
        "      loss = self._interior_loss(xs)\n",
        "\n",
        "      if self.bdy_wt_r > 1e-6:\n",
        "        ## For 1st order PDE\n",
        "        loss +=  self.bdy_wt_r*self._boundary_loss(tensor(self.data.xbs_r))\n",
        "        ## For 2nd order PDE\n",
        "        # loss +=  self.bdy_wt_r*self._boundary_loss(tensor(self.data.xbs_r)) + self.bdy_wt_l*self._boundary_loss(tensor(self.data.xbs_l), bc = 0.0)\n",
        "      return loss\n",
        "\n",
        "def L2_loss(model,data):\n",
        "      u = model(tensor(data))[0]\n",
        "      v = model(tensor(data))[1]\n",
        "      loss = torch.mean((u - f_exact(tensor(data)))**2 + (v - g_exact(tensor(data)))**2)\n",
        "      return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbDhTxpN1ncv"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQih1AcM1ltd"
      },
      "outputs": [],
      "source": [
        "def _epoch_train(model, loss_fn, optimizer, batch):\n",
        "    train_loss = 0.0\n",
        "    L2 = 0.0\n",
        "    dl = DataLoader(loss_fn.data, batch_size=batch, shuffle=True)\n",
        "\n",
        "    for xs in dl:\n",
        "        xs = xs.float().requires_grad_(True)\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(xs)\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "        loss_l2 = L2_loss(model, xs)\n",
        "        train_loss += loss.item()\n",
        "        L2 += loss_l2\n",
        "    L2 /= len(dl)\n",
        "    train_loss /= len(dl)\n",
        "    return train_loss, L2\n",
        "\n",
        "\n",
        "def train(model, loss_fn, optimizer, batch=32, n_epochs=1000, n_skip=100):\n",
        "    losses = []\n",
        "    L2 = []\n",
        "    data = loss_fn.data\n",
        "    for epoch in range(n_epochs):\n",
        "        loss = _epoch_train(model, loss_fn, optimizer, batch)\n",
        "        train_loss = loss[0]\n",
        "        losses.append(train_loss)\n",
        "        loss_2 = loss[1]\n",
        "        loss_2 = loss_2.detach().numpy()\n",
        "        L2.append(loss_2)\n",
        "\n",
        "        if n_skip > 0:\n",
        "            if epoch % n_skip == 0:\n",
        "                print(f'Epoch {epoch}/{n_epochs}: Loss = {train_loss}, L2 Loss = {loss_2}')\n",
        "    return np.array(losses), np.array(L2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoHqLPd11sGb"
      },
      "source": [
        "### Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0DgyD447jwq"
      },
      "outputs": [],
      "source": [
        "def plot_solution(model, data):\n",
        "  us = model(tensor(data.xs))[:,0].unsqueeze(1)\n",
        "  vs = model(tensor(data.xs))[:,1].unsqueeze(1)\n",
        "  us = us.squeeze().detach().cpu().numpy()\n",
        "  vs = vs.squeeze().detach().cpu().numpy()\n",
        "  xs = data.xs.reshape(data.xs.size)\n",
        "  us_exact = f_exact(xs)\n",
        "  vs_exact = g_exact(xs)\n",
        "  plt.plot(xs, us_exact, 'k-', lw=5, label='Exact $\\lambda$')\n",
        "  plt.plot(xs, us, 'b-', lw=2, label='DNN $\\lambda$')\n",
        "  plt.plot(xs, vs_exact, 'k-', lw=5, label='Exact $\\\\nu$')\n",
        "  plt.plot(xs, vs, 'r-', lw=2, label='DNN $\\\\nu$')\n",
        "  # plt.xlim(0,1)\n",
        "  plt.xlabel('r')\n",
        "  plt.ylabel('$e^{-\\lambda}$')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "def plot_soln(model, data):\n",
        "  us = model(tensor(data.xs))[:,0].unsqueeze(1)\n",
        "  vs = model(tensor(data.xs))[:,1].unsqueeze(1)\n",
        "  us = us.squeeze().detach().cpu().numpy()\n",
        "  vs = vs.squeeze().detach().cpu().numpy()\n",
        "  xs = data.xs.reshape(data.xs.size)\n",
        "  us_exact = f_exact(xs)\n",
        "  vs_exact = g_exact(xs)\n",
        "  plt.plot(1-1/xs , us_exact, 'k-', lw=5, label='Exact $\\lambda$')\n",
        "  plt.plot((1-1/xs) , us, 'b-', lw=2, label='DNN $\\lambda$')\n",
        "  plt.plot(1-1/xs , vs_exact, 'k-', lw=5, label='Exact $\\\\nu$')\n",
        "  plt.plot((1-1/xs) , vs, 'r-', lw=2, label='DNN $\\\\nu$')\n",
        "  plt.xlabel('1-1/r')\n",
        "  plt.ylabel('$e^{-\\lambda}$')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqhy6HTAJiuU",
        "outputId": "f6a156da-8825-467b-d8c8-a7d8660ef669"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/3000: Loss = 37.45982587337494, L2 Loss = 0.6865988969802856\n",
            "Epoch 500/3000: Loss = 2.637889906509372e-13, L2 Loss = 0.0024481855798512697\n",
            "Epoch 1000/3000: Loss = 1.2443153103447457e-06, L2 Loss = 0.0024486826732754707\n",
            "Epoch 1500/3000: Loss = 2.7921955592091763e-07, L2 Loss = 0.0024478512350469828\n"
          ]
        }
      ],
      "source": [
        "# def loss():\n",
        "\n",
        "n_interior = 100\n",
        "train_data = UnitInterval(n_interior)\n",
        "\n",
        "dnn_pinn = DNN(dim_in=1, dim_out=2, n_layers=12, n_neurons=250)\n",
        "\n",
        "bdy_wt = 1.0\n",
        "loss_pinn = Residual_PINN(dnn_pinn, train_data)\n",
        "lr = 1e-3\n",
        "optimizer_pinn = torch.optim.Adam(dnn_pinn.parameters(), lr=lr)\n",
        "batch = 25\n",
        "epochs = 3000\n",
        "n_skip = 500\n",
        "\n",
        "loss_pinn_1 = train(dnn_pinn, loss_pinn, optimizer_pinn, batch=batch, n_epochs=epochs, n_skip=n_skip)\n",
        "loss_pinn_2 = loss_pinn_1[0]\n",
        "L2_pinn = loss_pinn_1[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NjktDOyw8hl"
      },
      "outputs": [],
      "source": [
        "# dnn_ritz = DNN(dim_in=1, dim_out=1, n_layers=3, n_neurons=10,activation=torch.nn.Tanh())\n",
        "# loss_ritz = Residual_Deep_Ritz(dnn_ritz, train_data, f)\n",
        "# optimizer_ritz = torch.optim.SGD(dnn_ritz.parameters(), lr=lr)\n",
        "# loss_ritz_1 = train(dnn_ritz, loss_ritz, optimizer_ritz, batch=batch, n_epochs=epochs, n_skip=n_skip)\n",
        "# loss_ritz_2 = loss_ritz_1[0]\n",
        "# L2_ritz = loss_ritz_1[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG4QUjyI4i-O"
      },
      "outputs": [],
      "source": [
        "n_epochs = np.arange(epochs)\n",
        "# plt.loglog(n_epochs, L2_pinn, 'r-', linewidth=2,label='L2_pinn')\n",
        "plt.loglog(n_epochs, loss_pinn_2, 'b-', linewidth=2,label='PINN')\n",
        "# plt.loglog(n_epochs, L2_ritz, 'r--', linewidth=2,label='L2_ritz')\n",
        "# plt.loglog(n_epochs, loss_ritz_2, 'g', linewidth=2,label='Deep Ritz')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZvsGh1XCqjH"
      },
      "outputs": [],
      "source": [
        "plot_solution(dnn_pinn, train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGhFXOfoekpi"
      },
      "outputs": [],
      "source": [
        "plot_soln(dnn_pinn, train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPpeufOqBlWP"
      },
      "outputs": [],
      "source": [
        "def f_new(u,xt,model=dnn_pinn,data=train_data):\n",
        "  us = model(tensor(data.xs))\n",
        "  us = us.squeeze().detach().cpu().numpy()\n",
        "  return u*(1/us-1)/xt\n",
        "\n",
        "# n_interior = 1000\n",
        "# train_data = UnitInterval(n_interior)\n",
        "\n",
        "dnn_nu = DNN(dim_in=1, dim_out=1, n_layers=3, n_neurons=10, activation=torch.nn.Tanh())\n",
        "\n",
        "# bdy_wt = 10.0\n",
        "loss_nu = Residual_PINN(dnn_nu, train_data, f)\n",
        "lr = 1e-3\n",
        "optimizer_nu = torch.optim.SGD(dnn_nu.parameters(), lr=lr)\n",
        "batch = 250\n",
        "epochs = 4000\n",
        "n_skip = 400\n",
        "\n",
        "loss_nu = train(dnn_nu, loss_nu, optimizer_nu, batch=batch, n_epochs=epochs, n_skip=n_skip)\n",
        "loss_nu_0 = loss_nu[0]\n",
        "L2_pinn = loss_nu[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dskk92YOE4Sa"
      },
      "outputs": [],
      "source": [
        "plot_solution(dnn_nu, train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNc_GQ1dDdtL"
      },
      "outputs": [],
      "source": [
        "plot_soln(dnn_nu, train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lp7q1kEFxTgz"
      },
      "outputs": [],
      "source": [
        "# plot_soln(dnn_ritz, train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PALQAeKlVYts"
      },
      "outputs": [],
      "source": [
        "# n_test = 111\n",
        "# test_data = UnitInterval(n_test)\n",
        "# loss_test = Residual(dnn, test_data, f, bdy_wt=0.0)\n",
        "# test_loss = loss_test(tensor(loss_test.data.xs))\n",
        "# plot_solution(dnn, test_data)\n",
        "# plot_solution "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXVIBWVYJ08e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DwbpO-PPR6O"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyP3qnguoUnHZ5XLTv974OtJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}